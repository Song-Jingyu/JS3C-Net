{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from utils import config\n",
    "# cfg = get_parser()\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import importlib\n",
    "import logging\n",
    "import shutil\n",
    "import spconv\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from utils.evaluate_completion import get_eval_mask\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import models.model_utils as model_utils\n",
    "from utils.np_ioueval import iouEval\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "args = config.cfg\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticKITTI\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "dtype = torch.float32  # Tensor type to be used\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['gpu']\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-6\n",
    "MOMENTUM_ORIGINAL = 0.5\n",
    "MOMENTUM_DECCAY = 0.5\n",
    "BN_MOMENTUM_MAX = 0.001\n",
    "NUM_CLASS_SEG = args['DATA']['classes_seg']\n",
    "NUM_CLASS_COMPLET = args['DATA']['classes_completion']\n",
    "\n",
    "exp_name = args['log_dir']\n",
    "\n",
    "seg_head = importlib.import_module('models.'+args['Segmentation']['model_name'])\n",
    "seg_model = seg_head.get_model\n",
    "\n",
    "complet_head = importlib.import_module('models.'+args['Completion']['model_name'])\n",
    "complet_model = complet_head.get_model\n",
    "\n",
    "print(args['DATA']['dataset'])\n",
    "if args['DATA']['dataset'] == 'SemanticKITTI':\n",
    "    dataset = importlib.import_module('kitti_dataset')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/jingyu/Jingyu-SSD1/carla_dataset'\n",
    "\n",
    "coordinate_type = \"cartesian\"\n",
    "cylindrical = coordinate_type==\"cylindrical\"\n",
    "T = 1 #single sweep\n",
    "B = args['TRAIN']['batch_size'] # Matching paper\n",
    "model_name = 'JS3CNet'\n",
    "writer = SummaryWriter(\"./Runs/\" + model_name)\n",
    "experiment_dir = \"./Runs/\" + model_name\n",
    "from dataset import CarlaDataset\n",
    "\n",
    "carla_ds_train = CarlaDataset(directory=os.path.join(data_dir, 'Train'), device=device, num_frames=T, cylindrical=cylindrical, config=args)\n",
    "carla_ds_val = CarlaDataset(directory=os.path.join(data_dir, 'Val'), device=device, num_frames=T, cylindrical=cylindrical, config=args, split='Val')\n",
    "\n",
    "logger = logging.getLogger(\"Model\")\n",
    "logger.setLevel(logging.INFO)\n",
    "def log_string(str):\n",
    "    logger.info(str)\n",
    "    print(str)\n",
    "with open(os.path.join(experiment_dir, 'args.txt'), 'w') as f:\n",
    "    json.dump(args, f, indent=2)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('%s/train.txt'%(experiment_dir))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class J3SC_Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.seg_head = seg_model(args)\n",
    "        self.complet_head = complet_model(args)\n",
    "        self.voxelpool = model_utils.VoxelPooling(args)\n",
    "        self.seg_sigma = nn.Parameter(torch.Tensor(1).uniform_(0.2, 1), requires_grad=True)\n",
    "        self.complet_sigma = nn.Parameter(torch.Tensor(1).uniform_(0.2, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seg_inputs, complet_inputs, _ = x\n",
    "\n",
    "        '''Segmentation Head'''\n",
    "        seg_feature = seg_inputs['seg_features']\n",
    "        # print(f'seg inputs is {seg_feature}')\n",
    "        seg_output, feat = self.seg_head(seg_inputs)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        '''Completion Head'''\n",
    "        coords = complet_inputs['complet_coords']\n",
    "        coords = coords[:, [0, 3, 2, 1]]\n",
    "        \n",
    "\n",
    "        # if args['DATA']['dataset'] == 'SemanticKITTI':\n",
    "        #     coords[:, 3] += 1  # TODO SemanticKITTI will generate [256,256,31]\n",
    "        # elif args['DATA']['dataset'] == 'SemanticPOSS':\n",
    "        #     coords[:, 3][coords[:, 3] > 31] = 31\n",
    "\n",
    "        # print(f'JS3CNet feat is {feat}')\n",
    "        if args['Completion']['feeding'] == 'both':\n",
    "            feeding = torch.cat([seg_output, feat],1)\n",
    "        elif args['Completion']['feeding'] == 'feat':\n",
    "            feeding = feat\n",
    "        else:\n",
    "            feeding = seg_output\n",
    "        features = self.voxelpool(invoxel_xyz=complet_inputs['complet_invoxel_features'][:, :, :-1],\n",
    "                                    invoxel_map=complet_inputs['complet_invoxel_features'][:, :, -1].long(),\n",
    "                                    src_feat=feeding,\n",
    "                                    voxel_center=complet_inputs['voxel_centers'])\n",
    "        if self.args['Completion']['no_fuse_feat']:\n",
    "            features[...] = 1\n",
    "            features = features.detach()\n",
    "        batch_complet = spconv.SparseConvTensor(features.float(), coords.int(), args['Completion']['full_scale'], args['TRAIN']['batch_size'])\n",
    "        batch_complet = dataset.sparse_tensor_augmentation(batch_complet, complet_inputs['state'])\n",
    "\n",
    "        if args['GENERAL']['debug']:\n",
    "            model_utils.check_occupation(complet_inputs['complet_input'], batch_complet.dense())\n",
    "        # print(batch_complet.dense()[0,:,:,:,:])\n",
    "        complet_output = self.complet_head(batch_complet)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return seg_output, complet_output, [self.seg_sigma, self.complet_sigma]\n",
    "\n",
    "def bn_momentum_adjust(m, momentum):\n",
    "    if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "        m.momentum = momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 (1/100):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4050 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.99861684327942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.42, SLoss 3.40, CAcc 0.04, SAcc 0.01:   0%|          | 1/4050 [00:11<13:01:55, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9913462307138861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.33, SLoss 3.32, CAcc 0.07, SAcc 0.02:   0%|          | 2/4050 [00:13<6:22:51,  5.67s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9907650579067464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.28, SLoss 3.20, CAcc 0.11, SAcc 0.02:   0%|          | 3/4050 [00:14<4:15:21,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9898318174017505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.23, SLoss 3.14, CAcc 0.16, SAcc 0.05:   0%|          | 4/4050 [00:16<3:15:37,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9955102745639786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.17, SLoss 3.20, CAcc 0.18, SAcc 0.09:   0%|          | 5/4050 [00:18<2:50:44,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.992503748125937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.15, SLoss 3.04, CAcc 0.22, SAcc 0.15:   0%|          | 6/4050 [00:19<2:30:56,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9913773635262477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.10, SLoss 3.07, CAcc 0.23, SAcc 0.18:   0%|          | 7/4050 [00:21<2:17:48,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9966805845511483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.07, SLoss 3.06, CAcc 0.28, SAcc 0.22:   0%|          | 8/4050 [00:22<2:06:15,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.989004157438707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.04, SLoss 3.02, CAcc 0.24, SAcc 0.28:   0%|          | 9/4050 [00:24<1:58:57,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9930778428041029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.02, SLoss 3.03, CAcc 0.26, SAcc 0.26:   0%|          | 10/4050 [00:25<1:54:18,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9946706829635636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 3.00, SLoss 2.95, CAcc 0.30, SAcc 0.34:   0%|          | 11/4050 [00:27<1:52:44,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9967781654011902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLoss 2.97, SLoss 2.94, CAcc 0.30, SAcc 0.34:   0%|          | 12/4050 [00:29<2:44:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.064\n",
      "[IOU EVAL] IGNORE:  []\n",
      "[IOU EVAL] INCLUDE:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24]\n",
      "[IOU EVAL] IGNORE:  []\n",
      "[IOU EVAL] INCLUDE:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/675 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9980802761584389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/675 [00:07<38:35,  3.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9923423759129527\n",
      "Alignment rate:  0.9943170683911424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/675 [00:08<25:44,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9975318133079771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/675 [00:11<17:21,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9972457927459041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/675 [00:11<14:48,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9981381417845506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/675 [00:12<13:24,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.999315950426525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/675 [00:13<12:15,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9996904503946757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 9/675 [00:14<11:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9891904163174071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 10/675 [00:15<11:22,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9715293873456257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/675 [00:16<11:05,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9500948084216032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/675 [00:17<10:56,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment rate:  0.9788885149033247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/675 [00:17<16:15,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ========================== COMPLETION RESULTS ==========================  \n",
      "IoU class 1 [fences] = 0.973\n",
      "IoU class 2 [other] = 0.434\n",
      "IoU class 3 [pedestrian] = 0.000\n",
      "IoU class 4 [pole] = 0.048\n",
      "IoU class 5 [road-lines] = 0.038\n",
      "IoU class 6 [road] = 0.000\n",
      "IoU class 7 [sidewalk] = 7.304\n",
      "IoU class 8 [vegetation] = 3.583\n",
      "IoU class 9 [vehicle] = 1.702\n",
      "IoU class 10 [wall] = 0.254\n",
      "IoU class 11 [traffic-sign] = 0.000\n",
      "IoU class 12 [sky] = 0.005\n",
      "IoU class 13 [ground] = 0.000\n",
      "IoU class 14 [bridge] = 0.000\n",
      "IoU class 15 [rail-track] = 0.000\n",
      "IoU class 16 [guard-rail] = 0.000\n",
      "IoU class 17 [traffic-light] = 0.000\n",
      "IoU class 18 [static] = 0.000\n",
      "IoU class 19 [dynamic] = 0.000\n",
      "IoU class 20 [water] = 0.000\n",
      "IoU class 21 [terrain] = 0.000\n",
      "IoU class 22 [extra-1] = 12.227\n",
      "IoU class 23 [extra-2] = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "24",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_75663/29041394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 log_string('IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(\n\u001b[0;32m--> 144\u001b[0;31m                     i=i, class_str=class_strings[class_inv_remap[i]], jacc=jacc*100))\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# compute remaining metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 24"
     ]
    }
   ],
   "source": [
    "classifier = J3SC_Net(args).cuda()\n",
    "criteria = model_utils.Loss(args).cuda()\n",
    "\n",
    "train_dataloader = DataLoader(carla_ds_train, num_workers=args['TRAIN']['train_workers'], batch_size=args['TRAIN']['batch_size'], shuffle=True, collate_fn=seg_head.Merge, pin_memory=True, drop_last=True, worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "seg_labelweights = torch.Tensor(carla_ds_train.seg_labelweights).cuda()\n",
    "compl_labelweights = torch.Tensor(carla_ds_train.compl_labelweights).cuda()\n",
    "\n",
    "val_dataloader = DataLoader(carla_ds_val, batch_size=args['TRAIN']['batch_size'], num_workers=args['TRAIN']['train_workers'], shuffle=False, collate_fn=seg_head.Merge, pin_memory=True, drop_last=True, worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "\n",
    "\n",
    "training_epochs = args['TRAIN']['epochs']\n",
    "# training_epoch = model_utils.checkpoint_restore(classifier, experiment_dir, True, train_from=args['TRAIN']['train_from'])\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args['TRAIN']['learning_rate'], weight_decay=1e-4)\n",
    "\n",
    "global_epoch = 0\n",
    "best_iou_sem_complt = 0\n",
    "best_iou_complt = 0\n",
    "best_iou_seg = 0\n",
    "kitti_config = yaml.safe_load(open('opt/carla.yaml', 'r'))\n",
    "class_strings = kitti_config[\"labels\"]\n",
    "\n",
    "\n",
    "seg_label_to_cat = kitti_config[\"class_strings\"]\n",
    "class_inv_remap = kitti_config[\"learning_map_inv\"]\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    classifier.train()\n",
    "    log_string('\\nEpoch %d (%d/%s):' % (global_epoch, epoch + 1, training_epochs))\n",
    "    lr = max(args['TRAIN']['learning_rate'] * (args['TRAIN']['lr_decay'] ** (epoch // args['TRAIN']['decay_step'])), LEARNING_RATE_CLIP)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    momentum = max(MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // args['TRAIN']['decay_step'])), BN_MOMENTUM_MAX)\n",
    "    if momentum < 0.01:\n",
    "        momentum = 0.01\n",
    "    classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            # seg_label = batch\n",
    "            # print(len(batch[0]))\n",
    "            seg_label = batch[0]['seg_labels']\n",
    "            complet_label = batch[1]['complet_labels']\n",
    "            invalid_voxels = batch[1]['complet_invalid']\n",
    "            seg_pred, complet_pred, sigma = classifier(batch)\n",
    "            \n",
    "            seg_label = seg_label.cuda()\n",
    "            complet_label = complet_label.cuda()\n",
    "            loss, loss_seg, loss_complet = criteria(seg_pred, seg_label, seg_labelweights,\n",
    "                                                        complet_pred, complet_label, compl_labelweights,\n",
    "                                                        invalid_voxels, sigma)\n",
    "\n",
    "            '''Evaluation in trianing'''\n",
    "            pred_choice_complet = complet_pred[-1].data.max(1)[1].to('cpu')\n",
    "            complet_label = complet_label.to('cpu')\n",
    "            complet_label[invalid_voxels==1] = 255\n",
    "            correct_complet = pred_choice_complet.eq(complet_label.long().data).to('cpu')[(complet_label!=0)&(complet_label!=255)].sum()\n",
    "\n",
    "            pred_choice_seg = seg_pred.data.max(1)[1].to('cpu')\n",
    "            seg_label = seg_label.to('cpu')\n",
    "            correct_seg = pred_choice_seg.eq(seg_label.long().data).to('cpu').sum()\n",
    "\n",
    "            batch_loss = loss.cpu().item()\n",
    "            train_loss += batch_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1000 == 0 and i > 0:\n",
    "                torch.save(classifier.state_dict(), '%s/model_latest.pth' % experiment_dir)\n",
    "\n",
    "            pbar.set_description('CLoss %.2f, SLoss %.2f, CAcc %.2f, SAcc %.2f' %\n",
    "                                    (loss_complet.item(),\n",
    "                                    loss_seg.item(),\n",
    "                                    correct_complet.item() / float(complet_label[(complet_label!=0)&(complet_label!=255)].size()[0]),\n",
    "                                    correct_seg.item() / float(seg_label.size()[0])))\n",
    "            writer.add_scalar(model_name + '/Loss/Completion', loss_complet.item(), train_count)\n",
    "            writer.add_scalar(model_name + '/Loss/Segmentation', loss_seg.item(), train_count)\n",
    "            writer.add_scalar(model_name + '/Accuracy/Completion', correct_complet.item() / float(complet_label[(complet_label!=0)&(complet_label!=255)].size()[0]), train_count)\n",
    "            writer.add_scalar(model_name + '/Accuracy/Segmentation', correct_seg.item() / float(seg_label.size()[0]), train_count)\n",
    "            pbar.update(1)\n",
    "            train_count += args['TRAIN']['batch_size']\n",
    "            \n",
    "            if args['GENERAL']['debug'] and i > 10:\n",
    "                break\n",
    "        \n",
    "    log_string('Train Loss: %.3f' % (train_loss / len(train_dataloader)))\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        classifier.eval()\n",
    "        complet_evaluator = iouEval(NUM_CLASS_COMPLET, [])\n",
    "        seg_evaluator = iouEval(NUM_CLASS_SEG, [])\n",
    "        epsilon = np.finfo(np.float32).eps\n",
    "\n",
    "        with tqdm(total=len(val_dataloader)) as pbar:\n",
    "            for i, batch in enumerate(val_dataloader):\n",
    "                seg_label = batch[0]['seg_labels']\n",
    "                complet_label = batch[1]['complet_labels']\n",
    "                invalid_voxels = batch[1]['complet_invalid']\n",
    "                try:\n",
    "                    seg_pred, complet_pred, _ = classifier(batch)\n",
    "                except:\n",
    "                    print('Error in inference!!')\n",
    "                    continue\n",
    "\n",
    "                seg_label = seg_label.cuda()\n",
    "                complet_label = complet_label.cuda()\n",
    "\n",
    "                pred_choice_complet = complet_pred[-1].data.max(1)[1].to('cpu')\n",
    "                complet_label = complet_label.to('cpu')\n",
    "\n",
    "                pred_choice_seg = seg_pred.data.max(1)[1].to('cpu').data.numpy()\n",
    "                seg_label = seg_label.to('cpu').data.numpy()\n",
    "\n",
    "                complet_label = complet_label.data.numpy()\n",
    "                pred_choice_complet = pred_choice_complet.numpy()\n",
    "                invalid_voxels = invalid_voxels.data.numpy()\n",
    "                masks = get_eval_mask(complet_label, invalid_voxels)\n",
    "\n",
    "                target = complet_label[masks]\n",
    "                pred = pred_choice_complet[masks]\n",
    "\n",
    "                pred_choice_seg = pred_choice_seg[seg_label != -100]\n",
    "                seg_label = seg_label[seg_label != -100]\n",
    "                complet_evaluator.addBatch(pred.astype(int), target.astype(int))\n",
    "                seg_evaluator.addBatch(pred_choice_seg.astype(int), seg_label.astype(int))\n",
    "                pbar.update(1)\n",
    "\n",
    "                \n",
    "                if args['GENERAL']['debug'] and i > 10:\n",
    "                    break\n",
    "\n",
    "        log_string(\"\\n  ========================== COMPLETION RESULTS ==========================  \")\n",
    "        _, class_jaccard = complet_evaluator.getIoU()\n",
    "        m_jaccard = class_jaccard[1:].mean()\n",
    "\n",
    "        ignore = [0]\n",
    "        # print also classwise\n",
    "        for i, jacc in enumerate(class_jaccard):\n",
    "            if i not in ignore:\n",
    "                log_string('IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(\n",
    "                    i=i, class_str=class_strings[class_inv_remap[i]], jacc=jacc*100))\n",
    "\n",
    "        # compute remaining metrics.\n",
    "        conf = complet_evaluator.get_confusion()\n",
    "        precision = np.sum(conf[1:, 1:]) / (np.sum(conf[1:, :]) + epsilon)\n",
    "        recall = np.sum(conf[1:, 1:]) / (np.sum(conf[:, 1:]) + epsilon)\n",
    "        acc_cmpltn = (np.sum(conf[1:, 1:])) / (np.sum(conf) - conf[0, 0])\n",
    "        mIoU_ssc = m_jaccard\n",
    "\n",
    "        log_string(\"Precision =\\t\" + str(np.round(precision * 100, 2)) + '\\n' +\n",
    "                    \"Recall =\\t\" + str(np.round(recall * 100, 2)) + '\\n' +\n",
    "                    \"IoU Cmpltn =\\t\" + str(np.round(acc_cmpltn * 100, 2)) + '\\n' +\n",
    "                    \"mIoU SSC =\\t\" + str(np.round(mIoU_ssc * 100, 2)))\n",
    "\n",
    "        log_string(\"\\n  ========================== SEGMENTATION RESULTS ==========================  \")\n",
    "        _, class_jaccard = seg_evaluator.getIoU()\n",
    "        m_jaccard = class_jaccard.mean()\n",
    "        for i, jacc in enumerate(class_jaccard):\n",
    "            log_string('IoU class {i:} [{class_str:}] = {jacc:.3f}'.format(\n",
    "                i=i, class_str=seg_label_to_cat[i], jacc=jacc*100))\n",
    "        log_string('Eval point avg class IoU: %f' % (m_jaccard*100))\n",
    "\n",
    "        if best_iou_sem_complt < mIoU_ssc:\n",
    "            best_iou_sem_complt = mIoU_ssc\n",
    "        if best_iou_complt < acc_cmpltn:\n",
    "            best_iou_complt = acc_cmpltn\n",
    "        if best_iou_seg < m_jaccard:\n",
    "            best_iou_seg = m_jaccard\n",
    "            torch.save(classifier.state_dict(), '%s/model_segiou_%.4f_compltiou_%.4f_epoch%d.pth' % (experiment_dir, best_iou_seg, mIoU_ssc, epoch+1))\n",
    "\n",
    "        log_string('\\nBest segmentation IoU: %f' % (best_iou_seg * 100))\n",
    "        log_string('Best semantic completion IoU: %f' % (best_iou_sem_complt * 100))\n",
    "        log_string('Best completion IoU: %f' % (best_iou_complt * 100))\n",
    "        writer.add_scalar(model_name + '/Val/seg_iou', m_jaccard, epoch+1)\n",
    "        writer.add_scalar(model_name + '/Val/completion_iou', acc_cmpltn, epoch+1)\n",
    "        writer.add_scalar(model_name + '/Val/sem_completion_iou', mIoU_ssc, epoch+1)\n",
    "\n",
    "    global_epoch += 1\n",
    "log_string('Done!')\n",
    "writer.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06101282618436711328b81f67b356dccfc8ea185a33bf26c819a8f9eb824c27"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('cylinder': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
